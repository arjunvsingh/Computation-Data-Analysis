{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prove Distortion Function \n",
    "\n",
    "The distortion function is as follow:\n",
    "\n",
    ">J = $\\sum_{i=1}^m \\sum_{j=1}^k r^{ij}  {\\lvert\\lvert} x^i - {\\mu}^j {\\lvert\\lvert}^2$\n",
    "\n",
    "First, take a derivative respect to ${\\mu}^j$ equals to 0. We keep ${\\mu}^j$ to be fixed because we are looking at a j-th cluster. And first derivative equals 0 to find an optimum point.\n",
    "\n",
    ">$\\frac{\\mathrm{d}}{\\mathrm{d}\\mu^j}$ $\\sum_{i=1}^m \\sum_{j=1}^k r^{ij}  {\\lvert\\lvert} x^i - {\\mu}^j {\\lvert\\lvert}^2 = 0$\n",
    "\n",
    ">$\\quad  \\sum_{i=1}^m \\sum_{j=1}^k 2 r^{ij}  (x^i - {\\mu}^j ) (1)  = 0$\n",
    "\n",
    "Then, drop the constant and $\\sum_{j=1}^k$ because j-th cluster is being observed at a time from the equation.\n",
    "\n",
    ">$\\sum_{i=1}^m  r^{ij} x^i - r^{ij} {\\mu}^j = 0$\n",
    "\n",
    ">$\\sum_{i=1}^m  r^{ij} x^i =\\sum_{i=1}^m  r^{ij} {\\mu}^j $\n",
    "\n",
    "Expand the equation and represent data ${\\mu}^j$ as a function of $x^i$.\n",
    "\n",
    ">$\\frac{\\sum_{i=1}^m  r^{ij} x^i}{\\sum_{i=1}^m  r^{ij}} = {\\mu}^j $ (Proven)\n",
    "\n",
    "It is proven that $\\mu^j$ is the center of j-th cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Converges to a Local Optimum in Finite Steps\n",
    "\n",
    "Start with the dissimilarity function and contraint.\n",
    "\n",
    ">J = $\\sum_{i=1}^m \\sum_{j=1}^k r^{ij}  {\\lvert\\lvert} x^i - {\\mu}^j {\\lvert\\lvert}^2$\n",
    "\n",
    ">$r^{ij}=\\begin{cases}\n",
    "      1, & \\text{if the Ecludiean Distance decreases}\\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases} $\n",
    "\n",
    "\n",
    "The dissimilarity will reach its optimum point when $x^i$ is equal to ${\\mu}^j$ or J = 0. And it only calculates all the points that are belong to tje closest cluster j-th.\n",
    "\n",
    "\n",
    "Going back to the proven equation in 1.1,\n",
    "\n",
    ">$\\frac{\\sum_{i=1}^m  r^{ij} x^i}{\\sum_{i=1}^m  r^{ij}} = {\\mu}^j $\n",
    "\n",
    "\n",
    "The equation represents the average center of the data points within the j-th cluster. Cluster j-th is being observed at a time and it means the optimal solution will be a local minimum. Secondly, there are a finite number of points (m) within the cluster. As the algorithm iterates each point in a cluster, the algorithm will summing up all $x^i$ and divide it by the total number of points.Then plug in the center to dissimilarity function and continue until the dissimilarity score stops decreasing. There are a finite number of clusters with finite number of points and stops when the there is no changes on the dissimalirity score. As a result, the algorithm will converge in a finite steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 K-means by Hands\n",
    "\n",
    "To begin with, there are 5 points in the data\n",
    ">Point 1 = {2,2}<br>\n",
    ">Point 2 = {-1,1}<br>\n",
    ">Point 3 = {3,1}<br>\n",
    ">Point 4 = {0,-1}<br>\n",
    ">Point 5 = {-2,-2}<br>\n",
    "\n",
    "And there are 2 initial cluster\n",
    ">Cluster A = {2,1}\n",
    ">Cluster B = {-3,-1}\n",
    "\n",
    "\n",
    "\n",
    "#### (a) Show Cluster Assignment\n",
    "\n",
    "\n",
    "Iteration 1\n",
    "\n",
    "\n",
    "Cluster Assignment point 1<br>\n",
    ">min {|2 - (-3)| + |2 - (-1)|, |2-2|+|2- (-1)|}<br>\n",
    ">min {8,1} <br>\n",
    ">1 (Cluster B)\n",
    "\n",
    "Cluster Assignment point 2<br>\n",
    ">min {|(-1) - (-3)| + |1 - (-1)|, |-1-2|+|1-1|}<br>\n",
    ">min {4,3}<br>\n",
    ">3 (Cluster B)\n",
    "\n",
    "\n",
    "Cluster Assignment point 3 <br>\n",
    ">min {|3 - (-3)| + |1 - (-1)|, |3-2|+|1-1)|}<br>\n",
    ">min {8,1}<br>\n",
    ">1 (Cluster A)\n",
    "                           \n",
    "Cluster Assignment point 4 <br>\n",
    ">min {|0 - (-3)| + |-1 - (-1)|, |0-2|+|(-1) -1|}<br>\n",
    ">min {3,4}<br>\n",
    ">3 (Cluster A)\n",
    "                           \n",
    "Cluster Assignment point 5 <br>\n",
    ">min {|(-2) - (-3)| + |(-2) - (-1)|, |-2-2|+|2-1|}<br>\n",
    ">min {2,7}<br>\n",
    ">2 (Cluster A)\n",
    "\n",
    "Cluster Assignment - Iteration 1\n",
    "\n",
    "|Point|Cluster|\n",
    "|-----|-------|\n",
    "|1    |   B   |\n",
    "|2    |   B   |\n",
    "|3    |   B   |\n",
    "|4    |   A   |\n",
    "|5    |   A   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Show Location of the new center\n",
    "Center A = { (0 - 2)/2 , (-1-2)/2 }    = {-1, -1.5}<br>\n",
    "Center B = { (2 -1 +3)/3 , (2+1+1)/3 } = {4/3, 4/3}\n",
    "\n",
    "New Center - Iteration 1\n",
    "\n",
    "|  x  |  y  |Cluster|\n",
    "|-----|-----|-------|\n",
    "|-1   |-1.5 |   A   |\n",
    "|4/3  |4/3  |   B   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Will it terminate in one step?\n",
    "\n",
    "Point 2 has a close number during the cluster assignment, let's test if it is changing with the new centers\n",
    "\n",
    "Cluster Assignment point 2<br>\n",
    ">min {|(-1) - (-1)| + |1 - (-3/2)|, |-1-4/3|+|1-4/3|}<br>\n",
    ">min {5/2,8/3}<br>\n",
    ">5/2 (Cluster A)\n",
    "\n",
    "Point 2 assignment changes to Cluster A, the program will proceed to second iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Image Compression Using Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 K-medoids framework\n",
    "\n",
    "First, I generated a unique list of pixes from the image. The purpose of this step was to avoid duplicate pixel. And then,a number of clusters(K) was selected as starting centroids from the unique list.\n",
    "\n",
    "Secondly, each pixel was assign a label according to the distance between the pixel and cluster centroids. The assignment was based on the closest distance (smallest dissimilarity) to the cluster by using Euclidean distance.\n",
    "\n",
    "Next, within each cluster, the algorithm swapped the center of the cluster to find the smallest total dissimilarity with all available pixels within k-th cluster. To speed up the algorithm, all the points in the k-th cluster were made unique.\n",
    "\n",
    "The algorithm would terminate if the current dissimilarity score was dropping less than 20% from the old dissimalirity score.\n",
    "\n",
    "Finally, re-labeled all the points with the new centroids and replaced the pixel values in k-th cluster with k-th cluster centroid.\n",
    "\n",
    "### 2.2 Attach pictures - K-medoids\n",
    "\n",
    "<img src=\"Q2 Image Output\\out-kmedoids-beach-16.bmp\">\n",
    "Picture   : beach.bmp\n",
    "K         : 16\n",
    "Run time  : 75s ~ 1 min\n",
    "\n",
    "<img src=\"Q2 Image Output\\out-kmedoids-beach-2.bmp\">\n",
    "Picture   : beach.bmp\n",
    "K         : 2\n",
    "Run time  : 570s ~ 10 mins\n",
    "\n",
    "<img src=\"Q2 Image Output\\out-kmedoids-football-16.bmp\">\n",
    "Picture   : football.bmp\n",
    "K         : 16\n",
    "Run time  : 1242s ~ 20 mins\n",
    "\n",
    "<img src=\"Q2 Image Output\\out-kmedoids-football-2.bmp\">\n",
    "Picture   : football.bmp\n",
    "K         : 2\n",
    "Run time  : 12710s ~ 211 mins\n",
    "\n",
    "<img src=\"Q2 Image Output\\out-kmedoids-bugatti-16.bmp\">\n",
    "Picture   : bugatti.bmp\n",
    "K         : 16\n",
    "Run time  : 103s ~ 1.5 mins\n",
    "\n",
    "<img src=\"Q2 Image Output\\out-kmedoids-bugatti-2.bmp\">\n",
    "Picture   : bugatti.bmp\n",
    "K         : 2\n",
    "Run time  : 1225s ~ 20 mins\n",
    "\n",
    "\n",
    "K- Medoids Observations:\n",
    "\n",
    "|Picture  |Pixel Size |   K  | Time Taken(s)|\n",
    "|---------|-----------|------|--------------|\n",
    "|Beach    |320 x 214  |   16 |  75|\n",
    "|Beach    |320 x 214  |   2  |570|\n",
    "|Football |620 x 412  |   16 |1242|\n",
    "|Football |620 x 412  |   2  |12710|\n",
    "|Bugatti  |275 x 183  |   16 |103|\n",
    "|Bugatti  |275 x 183  |   2  |1225|\n",
    "\n",
    "1. Higher K value gives more colors to the image and it looks sharper. This happens because there are more color choices to represent the image.\n",
    "\n",
    "2. Higher K value tends to give a much faster computation speed than a lower k value since the algorithm for k-medoids is an exponential function. With more clusters, less number of points are processed during the \"swap\" step.\n",
    "\n",
    "3. In general, a higher size of image would run longer because there were more pixels to work on. However, the number of unique colors also play an important role. Less number of centroids swapping would take place.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.3 Random Cluster Initiation - K-Medoids\n",
    "\n",
    "Different results. Because the algorithm will converge to a local optimum and in my code, I put a condition where the centroid searching will be stop if the total dissimilarity is decreasing less than 20% per iteration.  A good cluster assigments would spread out evenly within the possible pixels, whereas a bad clustering are only focusing in a certain range of pixels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.4 K- means\n",
    "<img src=\"Q2 Image Output\\out-kmeans-beach-16.bmp\">\n",
    "\n",
    "Picture   : beach.bmp,\n",
    "K         : 16,\n",
    "Run time  : 16s\n",
    "\n",
    "<img src=\"Q2 Image Output\\out-kmeans-beach-2.bmp\">\n",
    "Picture   : beach.bmp,\n",
    "K         : 2,\n",
    "Run time  : 0.86s\n",
    "\n",
    "<img src=\"Q2 Image Output\\out-kmeans-football-16.bmp\">\n",
    "Picture   : beach.bmp,\n",
    "K         : 16,\n",
    "Run time  : 66s\n",
    "\n",
    "<img src=\"Q2 Image Output\\out-kmeans-football-2.bmp\">\n",
    "Picture   : beach.bmp,\n",
    "K         : 2,\n",
    "Run time  : 5.6s\n",
    "\n",
    "<img src=\"Q2 Image Output\\out-kmeans-bugatti-16.bmp\">\n",
    "Picture   : beach.bmp,\n",
    "K         : 16,\n",
    "Run time  : 16s\n",
    "\n",
    "<img src=\"Q2 Image Output\\out-kmeans-bugatti-2.bmp\">\n",
    "Picture   : beach.bmp,\n",
    "K         : 2,\n",
    "Run time  : 0.8s\n",
    "\n",
    "\n",
    "K-Means Observations:\n",
    "\n",
    "\n",
    "\n",
    "|Picture  |Pixel Size |   K  | Time Taken(s)|\n",
    "|---------|-----------|------|--------------|\n",
    "|Beach    |320 x 214  |   16 |     16  |\n",
    "|Beach    |320 x 214  |   2  |   0.86  |\n",
    "|Football |620 x 412  |   16 |   66    |\n",
    "|Football |620 x 412  |   2  |   5.6   |\n",
    "|Bugatti  |275 x 183  |   16 |   16    |\n",
    "|Bugatti  |275 x 183  |   2  |   0.8   |\n",
    "\n",
    "\n",
    "1. On higher K-values, K-means give a little sharper image than K-medoids. Especially around the shadow area on the image, thus it makes the image looks sharper. On lower K-values, the image colors are slighty different than k-medoids.\n",
    "\n",
    "2. K-means give a better color and closer to the original image. Because K-means can pick a pixel color that doesn't exist on the image that produces a lower dissimalirity score.\n",
    "\n",
    "3. K-means run faster than k-medoids because it doesn't need to do centroids swapping process, which take a lot of iterations.\n",
    "\n",
    "4. The K-means produce a similar result for each trial that eyes can't see the difference. The results are more consinstent in compare with K-medoids.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Spectral Clustering\n",
    "\n",
    "### 3.1 Method to Separate Two Moon\n",
    "\n",
    "Answer: Spectral Clustering. The two moon image needs to be separated based on the point connectivity. Even if the distance of points are closed but they are not connected, those points will not be clustered together. The premise is, distances between all connected point are roughly the same on the same scale. So we simply cluster the points which have similar simmilarity between each other\n",
    "\n",
    "On the other hand, K-means is not a good fit to separate the moon. Because K-means look for more compactnses within the data points. K-means algorithm will find a centroid that measures the closeness to other points and cluster them as one. The moon shape is very unique because the top and bottom part are not close but the data points are connected. Thus, K-means might split the month vertically/horizontally instead of following its shape\n",
    "\n",
    "\n",
    "### 3.2 Political Blog Dataset\n",
    "\n",
    "#### 3.2.a False Classification Rate\n",
    "\n",
    "Spectral Clustering Result\n",
    "<img src=\"Data\\spectral_clustering_result.png\">\n",
    "\n",
    "The false classification rate is 51.96%. Out of 1224 nodes, 588 nodes are labeled correctly and 636 nodes are labeled incorrectly.\n",
    "\n",
    "#### 3.2.b Perfomance\n",
    "\n",
    "Nodes Edges List\n",
    "<img src=\"Data\\edge_list.png\">\n",
    "\n",
    "As we can see from the picture above, the nodes are tighlty connected within each others. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA : Food Consumption in European Area\n",
    "\n",
    "### 4.1 PCA Setting\n",
    "\n",
    "To begin with, we want to remove all the rows that have empty values. And then separate the independent variables which starting from column number 1 to the end. The rows are M and the columns are N. Then we make an MxN matrix.\n",
    "\n",
    "Our goal is to find dimensions gives the most information or highest variance. This can be calculated by looking at the highest eigenvalues.\n",
    "\n",
    "<li>First find the mean for each feature on the given data points\n",
    "\n",
    "<li>And then find the diagonal Covariance matrix\n",
    "\n",
    "<li>From the covariance matrix, we can find the eigenvalues and the eigenvectors\n",
    "\n",
    "<li>Pair the largest eigenvalue with its vector and so on.\n",
    "\n",
    "<li>Compute reduced representation where Lambda 1 has the largest eiqenvalue\n",
    "    \n",
    "### 4.2  PCA Optimization\n",
    "\n",
    "Lecture page 12-16\n",
    "\n",
    "\n",
    "### 4.3 Top Two Principal Directions\n",
    "<img src=\"Data\\PCA1.png\">\n",
    "<img src=\"Data\\PCA2.png\">\n",
    "\n",
    "Observation:\n",
    "\n",
    "PC-1 : There are 20 features. Real coffe has no impact on PC1 as it shows a score of 0. Garlic (largest positive eigenvector) and olive oil are the only features that show positive relationship. The rest of the features are giving inverse relationship between the factor and variables. In PC1, Garlic has the biggest positive eigenvector and Tinned fruit has the biggest negative eigenvector.\n",
    "\n",
    "PC-2 : On the second PCA, Real coffee has the second largest negative relationship between the factor and variable. 8 features are giving positive eigenvectors. Garlichas minimal impact and Frozen fish has the highest in PC2\n",
    "\n",
    "\n",
    "### 4.4\n",
    "Foods Eigevector\n",
    "<img src=\"Data\\food_eigenvector.png\">\n",
    "\n",
    "\n",
    "\n",
    "Countries\n",
    "<img src=\"Data\\PCA-Part4.png\">\n",
    "\n",
    "\n",
    "From the eigenvector plot above, Olive Oil and Garlic are in quadrant I, Real Coffee is the only food in Quadrant IV and there are a balance mix for other foods in Quadrant II and III. \n",
    "\n",
    "\n",
    "When we translate this information to show plot by country, we find that England has a low consumption of Olive oil,  Garlic, and Real Coffee. As a result they are located at left most part of the graph. On the other hand, countries like Italy and Portugal have high consumptions of these foods.\n",
    "\n",
    "From cultural perspective this plot shows which country are closely related in food dishes. Italy is well know for its pasta and that's why they are on the right part of the graph. Same behavior with England, they are known to a country that consumes tea with biscuits as a compliment food.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
